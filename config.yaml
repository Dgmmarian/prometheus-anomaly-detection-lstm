# URL your Prometheus server
prometheus_url: "http://172.30.208.1:9090"

# A directory to preserve all artifacts: data, models, scalers and graphs.
artifacts_dir: "artifacts"

# PromQL queries to collect metrics
# The key is the column name in the final DataFrame.
queries:
  virtual_memory_free_bytes: 'windows_os_virtual_memory_free_bytes{job="Windows Exporter"}'
  system_threads: 'windows_system_threads{job="Windows Exporter"}'
  nvidia_smi_utilization_memory_ratio: nvidia_smi_utilization_memory_ratio{uuid="3714246e-aa22-1cf2-db00-bec4cab286ac"}
  nvidia_smi_utilization_gpu_ratio: nvidia_smi_utilization_gpu_ratio{uuid="3714246e-aa22-1cf2-db00-bec4cab286ac"}
  nvidia_smi_temperature_gpu: nvidia_smi_temperature_gpu{uuid="3714246e-aa22-1cf2-db00-bec4cab286ac"}
  nvidia_smi_power_draw_watts: nvidia_smi_power_draw_watts{uuid="3714246e-aa22-1cf2-db00-bec4cab286ac"}
  nvidia_smi_fan_speed_ratio: nvidia_smi_fan_speed_ratio{uuid="3714246e-aa22-1cf2-db00-bec4cab286ac"}
  cpu_usage_total_non_idle: 'sum by (instance, job) (rate(windows_cpu_time_total{mode!="idle", job=~"Windows Exporter|WMI Exporter"}[1m]))'
  system_processor_queue_length: 'windows_system_processor_queue_length{job="Windows Exporter"}'
  physical_memory_free_bytes: 'windows_os_physical_memory_free_bytes{job=~"Windows Exporter|WMI Exporter"}'
  network_packets_received_discarded_total: 'rate(windows_net_packets_received_discarded_total{job="Windows Exporter"}[1m])'
  network_packets_outbound_discarded_total: 'rate(windows_net_packets_outbound_discarded_total{job="Windows Exporter"}[1m])'
  network_packets_received_errors_total: 'rate(windows_net_packets_received_errors_total{job="Windows Exporter"}[1m])'
  network_packets_outbound_errors_total: 'rate(windows_net_packets_outbound_errors_total{job="Windows Exporter"}[1m])'
  system_context_switches_rate: 'rate(windows_system_context_switches_total{job="Windows Exporter"}[1m])'
  system_calls_rate: 'rate(windows_system_system_calls_total{job="Windows Exporter"}[1m])'
  thermalzone_temperature_celsius: 'windows_thermalzone_temperature_celsius{job=~"Windows Exporter|WMI Exporter"}'

# Data collection settings
data_settings:
  # For what period in the past to collect data (in hours).
  # If 0 is specified or not, and start_time_iso/end_time_iso is specified, they will be used.
  collection_period_hours: 336
  collection_periods_iso:
    - start: "2025-05-23T10:00:00"
      end: "2025-05-26T00:00:00"
    - start: "2025-05-26T22:00:00"
      end: "2025-06-01T04:00:00"
    - start: "2025-06-04T04:00:00"
      end: "2025-06-07T15:00:00"
  cache_chunk_hours: 1
  # Optionally, you can set specific start/end dates in ISO (YYYY-MM-DDTHH:MM:SS) format.
  # If they are set and collection_period_hours = 0 or not, they will have priority.
  # start_time_iso: "2025-05-31T10:00:00"
  # end_time_iso: "2025-05-31T11:00:00"

  # Data sampling step (in seconds or Prometheus format)'15s', '1m', '1h')
  step: "2m"

  # File name to save the final dataset
  # The file will also contain columns day_of_week and hour_of_day.
  output_filename: "prometheus_metrics_data.parquet"

preprocessing_settings:
  # File name with"raw" data (input for this script)
  # This is usually output_filename from the data_settings section.
  # If not stated, will be taken from data_settings.output_filename
  # input_filename: "prometheus_metrics_data.parquet" 

  # NaN filling strategy
  # Possible values:
  # - "ffill_then_bfill": First direct, then reverse filling
  # - "mean": filling-in
  # - "median": columnization
  # - "drop_rows": stripping
  # - "none": Do nothing with NaN (not recommended for most models)
  nan_fill_strategy: "ffill_then_bfill"

  # Type of skater for normalization/standardization
  # Possible values:"MinMaxScaler", "StandardScaler"
  scaler_type: "MinMaxScaler"

  # At this stage and when collecting data, temporary signs are added:
  # 'day_of_week' and'hour_of_day'

  # File name to save processed data
  processed_output_filename: "processed_metrics_data.parquet"

  # File name to save a trained skater
  scaler_output_filename: "fitted_scaler.joblib"

data_filtering_settings:
  # input_processed_filename: will be taken from preprocessing_setting.processed_output_filename
  # model_filename: will be taken from training_setting.model_output_filename
  # sequence_length: will be taken from training_setting.sequence_length
  # anomaly_threshold_mse: will be taken from real_time_anomaly_detection.anomaly_threshold_mse

  # Output file names for filtered sequences
  normal_sequences_output_filename: "normal_sequences.npy"
  anomalous_sequences_output_filename: "anomalous_sequences.npy"
  # Optional: Save all reconstruction errors for analysis
  # all_sequence_errors_output_filename: "all_sequence_errors.npy"

training_settings:
  # General training parameters
  # input_processed_filename: "processed_metrics_data.parquet"
  model_output_filename: "lstm_autoencoder_model.keras"
  sequence_length: 20
  train_split_ratio: 0.8
  epochs: 50
  batch_size: 64
  learning_rate: 0.001
  early_stopping_patience: 10
  lstm_units_encoder1: 64
  lstm_units_encoder2_latent: 32
  lstm_units_decoder1: 32
  lstm_units_decoder2: 64


real_time_anomaly_detection:
  # How often (in seconds) to poll Prometheus and perform detection
  query_interval_seconds: 30

  # Reconstruction error threshold (MSE) for declaring an anomaly.
  # !!! IMPORTANT: This value should be carefully selected based on analysis.
  # histogram of errors on validation / test"normal" data.
  # For example, the 95th or 99th percentile of these errors.
  # So far, it's a stub.
  # An example requires fine-tuning!
  anomaly_threshold_mse: 0.0025


  # Port where Prometheus exporter of this module will operate
  exporter_port: 8901 # Make sure the port is not occupied.

  # Prefix for metrics that this exporter will publish (optional)
  metrics_prefix: "anomaly_detector_"

  # Settings to request data for a single detection window:
  # Duration of the data step, as in data_settings (necessary to calculate the window)
  # If not specified, it will be taken from data_settings.step
  # data_step_duration: "30s" # for example,"30s", "1m"
