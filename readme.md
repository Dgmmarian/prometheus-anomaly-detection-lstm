# Prometheus Time Series Anomaly Detection with LSTM Autoencoder

This project implements a system for detecting anomalies in time series data collected from Prometheus. It uses an LSTM (Long Short-Term Memory) autoencoder model built with TensorFlow/Keras to learn normal patterns from your metrics and identify deviations. The system includes scripts for data collection, preprocessing, model training, and real-time anomaly detection with results exposed via a Prometheus exporter.

## Features

* **Data Collection:** Fetches time series data from a Prometheus instance for specified PromQL queries.
* **Preprocessing:** Handles missing values and normalizes/scales data for optimal model training.
* **LSTM Autoencoder Training:** Trains an LSTM autoencoder model to learn representations of normal metric behavior.
* **Real-time Anomaly Detection:** Continuously monitors new data, preprocesses it, and uses the trained model to detect anomalies.
* **Prometheus Exporter Integration:** Exposes key anomaly detection metrics (e.g., reconstruction error, anomaly flag, per-feature errors) that can be scraped by another Prometheus instance or monitored with tools like Grafana.
* **Configurable:** All stages are highly configurable via a central `config.yaml` file.


## Prerequisites

* Python 3.8+
* Pip (Python package installer)
* A running Prometheus instance (v2.x or later) that is scraping the metrics you want to analyze.
* (Optional) Exporters configured for your Prometheus to collect the desired metrics (e.g., `node_exporter`, `windows_exporter`).

## Setup & Installation

1.  **Clone the Repository:**
    ```bash
    git clone <your-repository-url>
    cd <repository-name>
    ```

2.  **Create a Virtual Environment (Recommended):**
    ```bash
    python -m venv venv
    source venv/bin/activate  # On Windows: venv\Scripts\activate
    ```

3.  **Install Python Dependencies:**
    A `requirements.txt` file should be included in the repository. Install dependencies using:
    ```bash
    pip install -r requirements.txt
    ```
    If `requirements.txt` is not present, you can create one after installing the necessary packages manually:
    ```bash
    pip install pandas numpy PyYAML scikit-learn tensorflow joblib requests prometheus_client matplotlib
    pip freeze > requirements.txt
    ```

4.  **Prometheus Setup:**
    Ensure your Prometheus server is running and accessible. The scripts will query this server based on the URL and PromQL queries defined in `config.yaml`. The example queries in `config.yaml` might use metrics from `windows_exporter`; adapt these to your own available metrics.

## Configuration (`config.yaml`)

The `config.yaml` file is central to running this project. It's divided into sections for each stage:

* **`prometheus_url`**: URL of your Prometheus server (e.g., `http://localhost:9090`).
* **`queries`**: A dictionary where keys are friendly names (used as column names and feature labels) and values are the PromQL queries.
    *Example:*
    ```yaml
    queries:
      virtual_memory_free_bytes: 'windows_os_virtual_memory_free_bytes{job="Windows Exporter"}'
      system_threads: 'windows_system_threads{job="Windows Exporter"}'
    ```
* **`data_settings`**: Parameters for the `data_collector.py` script.
    * `collection_period_hours`: How many hours of historical data to fetch.
    * `step`: The query resolution (e.g., `30s`, `1m`).
    * `output_filename`: Name of the Parquet file to save collected data.
* **`preprocessing_settings`**: Parameters for `preprocess_data.py`.
    * `nan_fill_strategy`: Method to handle missing values (e.g., `ffill_then_bfill`).
    * `scaler_type`: Type of scaler to use (`MinMaxScaler` or `StandardScaler`).
    * `processed_output_filename`: Output file for processed data.
    * `scaler_output_filename`: File to save the fitted scaler.
* **`training_settings`**: Parameters for `train_autoencoder.py`.
    * `model_output_filename`: File to save the trained Keras model.
    * `sequence_length`: Number of time steps in each input sequence for the LSTM.
    * `train_split_ratio`: Proportion of data for training.
    * `epochs`, `batch_size`, `learning_rate`: Standard training hyperparameters.
    * `early_stopping_patience`: Patience for early stopping (0 to disable).
    * `lstm_units_encoder1`, `lstm_units_encoder2_latent`, etc.: Define the LSTM autoencoder architecture.
* **`real_time_anomaly_detection`**: Parameters for `realtime_detector.py`.
    * `query_interval_seconds`: How often to fetch new data and run detection.
    * `anomaly_threshold_mse`: **Crucial parameter!** This Mean Squared Error threshold determines if a reconstruction is considered anomalous. It needs to be carefully tuned based on the histogram of reconstruction errors on your normal validation data (see `reconstruction_error_validation_histogram.png` generated by `train_autoencoder.py`).
    * `exporter_port`: Port for the Prometheus exporter of this detector.
    * `metrics_prefix`: Prefix for metrics exposed by the detector.

**Before running any script, review and customize `config.yaml` to match your environment and data.**

## Usage / Workflow

The project follows a sequential workflow:

**Step 1: Data Collection**
Collect historical data from your Prometheus instance.
```bash
python data_collector.py
````

This will create a Parquet file (e.g., `prometheus_metrics_data.parquet`) as specified in `config.yaml`.

**Step 2: Data Preprocessing**
Preprocess the collected data (handles NaNs, scales features).

```bash
python preprocess_data.py
```

This generates a processed Parquet file (e.g., `processed_metrics_data.parquet`) and saves the scaler (e.g., `fitted_scaler.joblib`).

**Step 3: Model Training**
Train the LSTM autoencoder model on the preprocessed data.

```bash
python train_autoencoder.py
```

This saves the trained model (e.g., `lstm_autoencoder_model.keras`) and generates plots for training history and reconstruction error distribution. Use the `reconstruction_error_validation_histogram.png` to help determine a suitable `anomaly_threshold_mse` for `config.yaml`.

**Step 4: Real-time Anomaly Detection**
Run the real-time detector. This script will continuously fetch new data, process it, and check for anomalies.

```bash
python realtime_detector.py
```

The detector will start a Prometheus exporter on the port specified in `config.yaml` (e.g., `http://localhost:8001/metrics`).

**Step 5: Monitoring (Prometheus & Grafana)**
Configure another Prometheus instance (or your main one) to scrape the metrics endpoint exposed by `realtime_detector.py`. You can then use Grafana or Prometheus's own UI to visualize:

  * `anomaly_detector_latest_reconstruction_error_mse`
  * `anomaly_detector_is_anomaly_detected` (0 for normal, 1 for anomaly)
  * `anomaly_detector_total_anomalies_count`
  * `anomaly_detector_feature_reconstruction_error_mse{feature_name="your_metric_alias"}` (per-feature errors)

## Interpreting Results

  * **Overall Anomaly Flag:** The `anomaly_detector_is_anomaly_detected` metric gives a quick overview.
  * **Reconstruction Error:** Monitor `anomaly_detector_latest_reconstruction_error_mse`. Spikes above your defined threshold indicate potential anomalies.
  * **Per-Feature Errors:** When an anomaly is detected, check the logs of `realtime_detector.py` for a breakdown of reconstruction errors per feature. Additionally, the `anomaly_detector_feature_reconstruction_error_mse{feature_name="..."}` metrics in Prometheus show which specific time series contributed most to the high reconstruction error, aiding in diagnosing the root cause.

## Customization & Extending

  * **Monitoring New Metrics:** Add new PromQL queries and friendly aliases to the `queries` section in `config.yaml`. You will need to retrain the model (Steps 1-3) to include these new metrics.
  * **Adjusting Time Windows:** Modify `collection_period_hours` (for historical data) and `sequence_length` (for model input) in `config.yaml`.
  * **Tuning Anomaly Threshold:** The `anomaly_threshold_mse` in `config.yaml` is critical. Adjust it based on the model's performance on your validation data and your tolerance for false positives/negatives.
  * **Model Architecture:** The LSTM autoencoder architecture can be modified within `train_autoencoder.py` (in the `build_lstm_autoencoder` function) or by making more parameters configurable in `config.yaml` under `training_settings`.
  * **Data Fetching in Real-time:** The `_fetch_data_window` method in `realtime_detector.py` can be adjusted if you have specific needs for how fresh data is queried.

## Troubleshooting

  * **Python Dependencies:** Ensure all packages in `requirements.txt` are installed in your active Python environment.
  * **Prometheus Connection:** Verify the `prometheus_url` in `config.yaml` is correct and your Prometheus server is accessible. Check Prometheus logs if data isn't being fetched.
  * **Data Issues:**
      * If `data_collector.py` or `realtime_detector.py` report "No data found," check your PromQL queries and ensure Prometheus is scraping the target metrics.
      * NaN issues after preprocessing: Review the `nan_fill_strategy` or inspect raw data for large gaps.
  * **Model Training:**
      * If loss doesn't decrease: try adjusting `learning_rate`, `batch_size`, model architecture, or ensure data is properly scaled.
      * Overfitting: If validation loss consistently increases while training loss decreases, try adding more regularization (e.g., Dropout layers), getting more data, or simplifying the model. `EarlyStopping` is enabled by default to help with this.
  * **Port in Use:** If `realtime_detector.py` fails to start the exporter, the specified `exporter_port` might be in use. Choose a different port in `config.yaml`.

## Contributing

Contributions are welcome\! Please feel free to open an issue or submit a pull request.
(Consider adding more specific guidelines if you wish, e.g., for coding style, testing.)

## License

This project is licensed under the MIT License